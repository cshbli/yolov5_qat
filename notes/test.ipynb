{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model state_dict only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'runs/train/exp_copy/weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = path + 'best.pt'\n",
    "ckpt = torch.load(model_file_path, map_location='cpu')\n",
    "torch.save(ckpt['model'], path + 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save state_dict only\n",
    "torch.save(ckpt['model'].state_dict(), path + 'model_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
      "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
      "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
      "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
      "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
      "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
      "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
      "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
      "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
      "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
      " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
      " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
      " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
      " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
      " 24      [17, 20, 23]  1    343485  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
      "YOLOv5m summary: 335 layers, 21190557 parameters, 21190557 gradients, 49.2 GFLOPs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.yolo import Model\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = Model('models/yolov5m.yaml', ch=3, nc=80, anchors=None).to(device)  # create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(path + \"model_state_dict.pt\", map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongbing/Projects/yolov5/models/yolo.py:219: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if augment:\n",
      "/home/hongbing/Projects/yolov5/models/yolo.py:126: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if profile:\n",
      "/home/hongbing/Projects/yolov5/models/yolo.py:130: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if visualize:\n",
      "/home/hongbing/Projects/yolov5/models/yolo.py:130: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if visualize:\n",
      "/home/hongbing/Projects/yolov5/models/yolo.py:126: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if profile:\n",
      "/home/hongbing/Projects/yolov5/models/yolo.py:71: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "# Input to the model\n",
    "x = torch.randn(batch_size, 3, 640, 640, requires_grad=True).to('cpu')\n",
    "torch_out = model(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  path + \"modified_add.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=13,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output0'], # the model's output names\n",
    "                  dynamic_axes=None    # variable length axes\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Checks\n",
    "model_onnx = onnx.load(path + \"modified_add.onnx\")  # load onnx model\n",
    "onnx.checker.check_model(model_onnx)  # check onnx model\n",
    "    \n",
    "onnx.save(model_onnx, path + \"modified_add.onnx\")\n",
    "\n",
    "# Simplify\n",
    "try:    \n",
    "    import onnxsim\n",
    "    \n",
    "    model_onnx, check = onnxsim.simplify(model_onnx)    \n",
    "    onnx.save(model_onnx, path + \"modified_add.onnx\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.36965e+00, -1.02389e+00,  3.68400e-01],\n",
       "          [-1.18623e-01, -1.05423e+00, -1.89012e-01],\n",
       "          [-3.45186e-01, -1.05892e+00,  1.57285e+00]],\n",
       "\n",
       "         [[ 7.25538e-01,  8.03549e-02,  3.21692e-01],\n",
       "          [ 2.48363e-01,  1.58795e+00, -1.19322e-01],\n",
       "          [ 2.41320e-01,  1.38358e+00, -1.34646e+00]],\n",
       "\n",
       "         [[ 3.61976e-02,  4.89959e-01,  8.89310e-03],\n",
       "          [ 1.17609e+00, -8.19324e-01,  4.88001e-01],\n",
       "          [-1.05081e+00,  9.68253e-01, -1.16043e-03]],\n",
       "\n",
       "         [[-2.76780e-01,  7.48895e-01,  1.18751e+00],\n",
       "          [ 2.81068e-02,  2.10410e+00, -8.68632e-01],\n",
       "          [ 5.69884e-01,  1.94860e+00,  2.85901e-02]],\n",
       "\n",
       "         [[ 7.82166e-01,  4.49962e-01,  1.31943e+00],\n",
       "          [-8.50161e-01,  1.90999e+00, -2.07294e-01],\n",
       "          [-1.91950e-01,  4.30786e-01, -3.34761e-02]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.randn(1, 2, 3, 3)\n",
    "tensor2 = torch.randn(1, 3, 3, 3)\n",
    "\n",
    "class Concat(nn.Module):\n",
    "    # Concatenate a list of tensors along dimension\n",
    "    def __init__(self, dimension=1):\n",
    "        super().__init__()\n",
    "        self.d = dimension\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return torch.cat(x, self.d)\n",
    "\n",
    "m = Concat()        \n",
    "\n",
    "m.f = 2\n",
    "m(tensor1, tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Concat()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m2 = nn.Sequential(m)\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat()\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for name, i in m2.named_children():\n",
    "    i.t = 2\n",
    "    print(i)\n",
    "    print(i.f)\n",
    "    print(i.t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42be7b6d852b9b2b1a0308f8b9cc6db97febf6d6b1b2a588c6bfd7d771418521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
